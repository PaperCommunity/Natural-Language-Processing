## 计算语言学

* [《Taylor’s Law for Human Linguistic Sequences》](https://github.com/PaperCommunity/Reinforcement-Learning/tree/master/ImitationLearning/Playing%20hard%20exploration%20games%20by%20watching%20YouTube)
<<<<<<< Updated upstream
  * **update**: 2019.01.24
  * **author**: [SPY-Ming](https://github.com/SPY-Ming)
  * **overview**: 使用泰勒规则来研究语言学所蕴含的规律
>>>>>>> Stashed changes

* [《What you can cram into a single $&!#* vector:Probing sentence embeddings for linguistic properties
》](https://github.com/PaperCommunity/Reinforcement-Learning/tree/master/ImitationLearning/Playing%20hard%20exploration%20games%20by%20watching%20YouTube)
<<<<<<< Updated upstream
  * **update**: 2019.02.01
  * **author**: [SPY-Ming](https://github.com/SPY-Ming)
  * **overview**: 现如今人们提出很多能训练出高质量句子的embedding，但我们还不知道他们具体抓取到了什么信息。Facebook团队使用10种推断任务来抓取句子的语言信息，并用他们研究用八种不同的训练方式得到的三种不同的编码器产生的embedding，揭露encoders和训练方法的神秘属性
>>>>>>> Stashed changes

* [《What you can cram into a single $&!#* vector:Probing sentence embeddings for linguistic properties》](https://github.com/PaperCommunity/Reinforcement-Learning/tree/master/ImitationLearning/Playing%20hard%20exploration%20games%20by%20watching%20YouTube)
<<<<<<< Updated upstream
  * **update**: 2019.02.01
  * **author**: [SPY-Ming](https://github.com/SPY-Ming)
  * **overview**: 现如今人们提出很多能训练出高质量句子的embedding，但我们还不知道他们具体抓取到了什么信息。Facebook团队使用10种推断任务来抓取句子的语言信息，并用他们研究用八种不同的训练方式得到的三种不同的编码器产生的embedding，揭露encoders和训练方法的神秘属性
>>>>>>> Stashed changes

* [《Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context》](https://github.com/PaperCommunity/Reinforcement-Learning/tree/master/ImitationLearning/Playing%20hard%20exploration%20games%20by%20watching%20YouTube)
<<<<<<< Updated upstream
  * **update**: 2019.02.01
  * **author**: [SPY-Ming](https://github.com/SPY-Ming)
  * **overview**: 由于人们对神经语言模型对语言文本使用情况一无所知，斯坦福大学的学者利用切除学习的方法研究文本在LSTM语言模型中的角色。该文章分析了文本单词被打乱、重置、丢弃后的困惑度增长情况。在Penn 和 Wiki数据集上，作者发现模型可以记忆约200tokens，但能够将附近的上下文（约50词）和远方的分开。在词序上，模型对附近的单词敏感，对远处的词序不敏感。作者还发现神经缓存模型有助于LSTM复制远处的文本。总之，该文章不但更好地提供了神经语言模型对文本的理解，还解释了基于缓存模型的成功。
>>>>>>> Stashed changes

